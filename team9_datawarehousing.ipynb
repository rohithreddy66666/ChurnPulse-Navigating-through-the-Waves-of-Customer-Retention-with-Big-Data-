{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dcb45eb-3ecc-41b8-9fae-d77a12f68b98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------------------------------------------------------+\n|key |value                                                               |\n+----+--------------------------------------------------------------------+\n|null|2.0,30.0,Female,39.0,14.0,5.0,18.0,Standard,Annual,932.0,17.0,1.0   |\n|null|3.0,65.0,Female,49.0,1.0,10.0,8.0,Basic,Monthly,557.0,6.0,1.0       |\n|null|4.0,55.0,Female,14.0,4.0,6.0,18.0,Basic,Quarterly,185.0,3.0,1.0     |\n|null|5.0,58.0,Male,38.0,21.0,7.0,7.0,Standard,Monthly,396.0,29.0,1.0     |\n|null|6.0,23.0,Male,32.0,20.0,5.0,8.0,Basic,Monthly,617.0,20.0,1.0        |\n|null|8.0,51.0,Male,33.0,25.0,9.0,26.0,Premium,Annual,129.0,8.0,1.0       |\n|null|9.0,58.0,Female,49.0,12.0,3.0,16.0,Standard,Quarterly,821.0,24.0,1.0|\n|null|10.0,55.0,Female,37.0,8.0,4.0,15.0,Premium,Annual,445.0,30.0,1.0    |\n|null|11.0,39.0,Male,12.0,5.0,7.0,4.0,Standard,Quarterly,969.0,13.0,1.0   |\n|null|12.0,64.0,Female,3.0,25.0,2.0,11.0,Standard,Quarterly,415.0,29.0,1.0|\n+----+--------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"KafkaReadTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from Kafka topic as a batch dataframe\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"52.27.188.150:9092\") \\\n",
    "    .option(\"subscribe\", \"bigdata-project2\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"\"\"{\"bigdata-project2\":{\"0\":10}}\"\"\") \\\n",
    "    .load()\n",
    "\n",
    "# Select and cast the key and value columns from Kafka's binary format\n",
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Show the first 10 messages\n",
    "df.show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701e0603-72a1-4587-a4b5-5d3f57c6dc8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of messages in the topic 'bigdata-project2': 505207\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"KafkaReadTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from Kafka topic\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"52.27.188.150:9092\") \\\n",
    "    .option(\"subscribe\", \"bigdata-project2\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Count the number of messages\n",
    "message_count = df.count()\n",
    "\n",
    "print(f\"Number of messages in the topic 'bigdata-project2': {message_count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f9fc0f7-5e90-42b2-9cca-79393ec4115a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+\n|CustomerID|Age|Gender|Tenure|Usage Frequency|Support Calls|Payment Delay|Subscription Type|Contract Length|Total Spend|Last Interaction|Churn|\n+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+\n|2         |30 |Female|39    |14             |5            |18           |Standard         |Annual         |932.0      |17              |1    |\n|3         |65 |Female|49    |1              |10           |8            |Basic            |Monthly        |557.0      |6               |1    |\n|4         |55 |Female|14    |4              |6            |18           |Basic            |Quarterly      |185.0      |3               |1    |\n|5         |58 |Male  |38    |21             |7            |7            |Standard         |Monthly        |396.0      |29              |1    |\n|6         |23 |Male  |32    |20             |5            |8            |Basic            |Monthly        |617.0      |20              |1    |\n|8         |51 |Male  |33    |25             |9            |26           |Premium          |Annual         |129.0      |8               |1    |\n|9         |58 |Female|49    |12             |3            |16           |Standard         |Quarterly      |821.0      |24              |1    |\n|10        |55 |Female|37    |8              |4            |15           |Premium          |Annual         |445.0      |30              |1    |\n|11        |39 |Male  |12    |5              |7            |4            |Standard         |Quarterly      |969.0      |13              |1    |\n|12        |64 |Female|3     |25             |2            |11           |Standard         |Quarterly      |415.0      |29              |1    |\n+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"KafkaReadTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from Kafka topic as a batch dataframe\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"52.27.188.150:9092\") \\\n",
    "    .option(\"subscribe\", \"bigdata-project2\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Select and cast the key and value columns from Kafka's binary format\n",
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Define the schema for your data\n",
    "schema = StructType([\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Tenure\", IntegerType(), True),\n",
    "    StructField(\"Usage Frequency\", IntegerType(), True),\n",
    "    StructField(\"Support Calls\", IntegerType(), True),\n",
    "    StructField(\"Payment Delay\", IntegerType(), True),\n",
    "    StructField(\"Subscription Type\", StringType(), True),\n",
    "    StructField(\"Contract Length\", StringType(), True),\n",
    "    StructField(\"Total Spend\", FloatType(), True),\n",
    "    StructField(\"Last Interaction\", IntegerType(), True),\n",
    "    StructField(\"Churn\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Split the 'value' column into individual columns\n",
    "df = df.selectExpr(\"split(value, ',') as data\")\n",
    "\n",
    "# Expand the 'data' array into separate columns\n",
    "for i in range(12):\n",
    "    df = df.withColumn(f\"col_{i+1}\", col(\"data\")[i])\n",
    "\n",
    "# Select and rename the columns based on the schema\n",
    "df = df.select(\n",
    "    col(\"col_1\").cast(IntegerType()).alias(\"CustomerID\"),\n",
    "    col(\"col_2\").cast(IntegerType()).alias(\"Age\"),\n",
    "    col(\"col_3\").alias(\"Gender\"),\n",
    "    col(\"col_4\").cast(IntegerType()).alias(\"Tenure\"),\n",
    "    col(\"col_5\").cast(IntegerType()).alias(\"Usage Frequency\"),\n",
    "    col(\"col_6\").cast(IntegerType()).alias(\"Support Calls\"),\n",
    "    col(\"col_7\").cast(IntegerType()).alias(\"Payment Delay\"),\n",
    "    col(\"col_8\").alias(\"Subscription Type\"),\n",
    "    col(\"col_9\").alias(\"Contract Length\"),\n",
    "    col(\"col_10\").cast(FloatType()).alias(\"Total Spend\"),\n",
    "    col(\"col_11\").cast(IntegerType()).alias(\"Last Interaction\"),\n",
    "    col(\"col_12\").cast(IntegerType()).alias(\"Churn\")\n",
    ")\n",
    "\n",
    "\n",
    "# Show the first 10 rows of the DataFrame with labels\n",
    "df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5daffa7-f974-42e0-8011-a94ade233ea8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 'df' has been written to S3 bucket at 's3://bigdata-team9-finalproject/processed_tables/dataset_with_labels.csv'.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a session using your Amazon S3 credentials\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='AKIARBUWNMP7DVA4YKPQ',\n",
    "    aws_secret_access_key='Cu4Hd5ktIvuFGQbnA3mzs9zjWiZD672uiaTXlp0S',\n",
    "    region_name='us-east-2'  # The region your S3 bucket is in\n",
    ")\n",
    "\n",
    "# Convert the DataFrame to CSV format as a string\n",
    "csv_buffer = StringIO()\n",
    "df.toPandas().to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Specify your S3 bucket and object key\n",
    "bucket_name = 'bigdata-team9-finalproject'\n",
    "object_key = 'processed_tables/dataset_with_labels.csv'\n",
    "\n",
    "# Upload the CSV data to S3\n",
    "s3 = session.client('s3')\n",
    "s3.put_object(Bucket=bucket_name, Key=object_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "# Print a success message\n",
    "print(f\"DataFrame 'df' has been written to S3 bucket at 's3://{bucket_name}/{object_key}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07c6915-2279-4e4f-a175-c27a93bdca68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+--------------+\n|CustomerID|Age|Gender|Tenure|Usage Frequency|Support Calls|Payment Delay|Subscription Type|Contract Length|Total Spend|Last Interaction|Churn|subscriptionid|\n+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+--------------+\n|2         |30 |Female|39    |14             |5            |18           |Standard         |Annual         |932.0      |17              |1    |1000000       |\n|3         |65 |Female|49    |1              |10           |8            |Basic            |Monthly        |557.0      |6               |1    |1000001       |\n|4         |55 |Female|14    |4              |6            |18           |Basic            |Quarterly      |185.0      |3               |1    |1000002       |\n|5         |58 |Male  |38    |21             |7            |7            |Standard         |Monthly        |396.0      |29              |1    |1000003       |\n|6         |23 |Male  |32    |20             |5            |8            |Basic            |Monthly        |617.0      |20              |1    |1000004       |\n|8         |51 |Male  |33    |25             |9            |26           |Premium          |Annual         |129.0      |8               |1    |1000005       |\n|9         |58 |Female|49    |12             |3            |16           |Standard         |Quarterly      |821.0      |24              |1    |1000006       |\n|10        |55 |Female|37    |8              |4            |15           |Premium          |Annual         |445.0      |30              |1    |1000007       |\n|11        |39 |Male  |12    |5              |7            |4            |Standard         |Quarterly      |969.0      |13              |1    |1000008       |\n|12        |64 |Female|3     |25             |2            |11           |Standard         |Quarterly      |415.0      |29              |1    |1000009       |\n+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+--------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add a new column 'subscriptionid' with a series of numbers\n",
    "df = df.withColumn(\"subscriptionid\", monotonically_increasing_id() + 1000000)\n",
    "\n",
    "# Show the first 10 rows of the DataFrame with the new 'subscriptionid' column\n",
    "df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a0832f-0bc3-4898-b252-61a9ab34ea9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customerdim DataFrame has been written to S3 bucket at 's3://bigdata-team9-finalproject/processed_tables/customerdim_original.csv'.\nchurnfacttable DataFrame has been written to S3 bucket at 's3://bigdata-team9-finalproject/processed_tables/churnfacttable_original.csv'.\nsubscriptiondim DataFrame has been written to S3 bucket at 's3://bigdata-team9-finalproject/processed_tables/subscriptiondim_original.csv'.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize a session using your Amazon S3 credentials\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='AKIARBUWNMP7DVA4YKPQ',\n",
    "    aws_secret_access_key='Cu4Hd5ktIvuFGQbnA3mzs9zjWiZD672uiaTXlp0S',\n",
    "    region_name='us-east-2'  # The region your S3 bucket is in\n",
    ")\n",
    "\n",
    "# Create customerdim DataFrame\n",
    "customerdim = df.select(\"CustomerID\", \"Age\", \"Gender\")\n",
    "\n",
    "# Create churnfacttable DataFrame\n",
    "churnfacttable = df.select(\n",
    "    \"CustomerID\", \"subscriptionid\", \"Churn\", \"Total Spend\", \"Usage Frequency\", \"Support Calls\",\n",
    "    \"Payment Delay\"\n",
    ")\n",
    "\n",
    "# Create subscriptiondim DataFrame\n",
    "subscriptiondim = df.select(\n",
    "    \"subscriptionid\", \"Contract Length\", \"Tenure\"\n",
    ")\n",
    "# Convert customerdim DataFrame to CSV format as a string\n",
    "csv_buffer_customerdim = StringIO()\n",
    "customerdim.toPandas().to_csv(csv_buffer_customerdim, index=False)\n",
    "\n",
    "# Specify your S3 bucket and object key for customerdim\n",
    "bucket_name = 'bigdata-team9-finalproject'\n",
    "object_key_customerdim = 'processed_tables/customerdim_original.csv'\n",
    "\n",
    "# Upload the CSV data for customerdim to S3\n",
    "s3_customerdim = session.client('s3')\n",
    "s3_customerdim.put_object(Bucket=bucket_name, Key=object_key_customerdim, Body=csv_buffer_customerdim.getvalue())\n",
    "\n",
    "# Print a success message for customerdim\n",
    "print(f\"customerdim DataFrame has been written to S3 bucket at 's3://{bucket_name}/{object_key_customerdim}'.\")\n",
    "\n",
    "# Repeat the above steps for churnfacttable and subscriptiondim\n",
    "# Convert churnfacttable DataFrame to CSV format as a string\n",
    "csv_buffer_churnfacttable = StringIO()\n",
    "churnfacttable.toPandas().to_csv(csv_buffer_churnfacttable, index=False)\n",
    "\n",
    "# Specify your S3 bucket and object key for churnfacttable\n",
    "object_key_churnfacttable = 'processed_tables/churnfacttable_original.csv'\n",
    "\n",
    "# Upload the CSV data for churnfacttable to S3\n",
    "s3_churnfacttable = session.client('s3')\n",
    "s3_churnfacttable.put_object(Bucket=bucket_name, Key=object_key_churnfacttable, Body=csv_buffer_churnfacttable.getvalue())\n",
    "\n",
    "# Print a success message for churnfacttable\n",
    "print(f\"churnfacttable DataFrame has been written to S3 bucket at 's3://{bucket_name}/{object_key_churnfacttable}'.\")\n",
    "\n",
    "# Convert subscriptiondim DataFrame to CSV format as a string\n",
    "csv_buffer_subscriptiondim = StringIO()\n",
    "subscriptiondim.toPandas().to_csv(csv_buffer_subscriptiondim, index=False)\n",
    "\n",
    "# Specify your S3 bucket and object key for subscriptiondim\n",
    "object_key_subscriptiondim = 'processed_tables/subscriptiondim_original.csv'\n",
    "\n",
    "# Upload the CSV data for subscriptiondim to S3\n",
    "s3_subscriptiondim = session.client('s3')\n",
    "s3_subscriptiondim.put_object(Bucket=bucket_name, Key=object_key_subscriptiondim, Body=csv_buffer_subscriptiondim.getvalue())\n",
    "\n",
    "# Print a success message for subscriptiondim\n",
    "print(f\"subscriptiondim DataFrame has been written to S3 bucket at 's3://{bucket_name}/{object_key_subscriptiondim}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "team9_datawarehousing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
